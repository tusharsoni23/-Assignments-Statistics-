{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0374f958-cfff-4b77-ba67-4b798c685ea2",
      "cell_type": "markdown",
      "source": "Here are detailed answers to all the theoretical questions on **Ensemble Learning** from your assignment:\n\n---\n\n### **1. Can we use Bagging for regression problems?**  \nYes, **Bagging (Bootstrap Aggregating)** can be used for regression problems. When applied to regression, it is called a **Bagging Regressor**. This technique trains multiple regression models (e.g., Decision Trees) on different bootstrap samples of the dataset and then **averages** their predictions. This reduces variance and improves generalization, making the model more stable and robust.\n\n---\n\n### **2. What is the difference between multiple model training and single model training?**  \n- **Single Model Training:** A single machine learning model is trained on the entire dataset. It may suffer from high variance (overfitting) or high bias (underfitting), depending on the model.  \n- **Multiple Model Training (Ensemble Learning):** Instead of relying on one model, multiple models are trained, and their predictions are combined to improve performance. Techniques such as Bagging, Boosting, and Stacking help reduce errors and increase generalization.\n\n---\n\n### **3. Explain the concept of feature randomness in Random Forest.**  \nIn **Random Forest**, each tree is trained not only on a random subset of the dataset but also on a **random subset of features** at each split. This is called **feature randomness**, and it helps to:  \n- Reduce correlation between trees  \n- Improve model generalization  \n- Prevent overfitting  \n\nUnlike traditional Decision Trees, which consider all features, Random Forest only selects a subset of features, making it more robust.\n\n---\n\n### **4. What is OOB (Out-of-Bag) Score?**  \nOOB Score is an internal cross-validation method used in Bagging and Random Forest. Since Bagging uses **bootstrap sampling** (random sampling with replacement), some data points are left out during training. These **Out-of-Bag (OOB) samples** can be used to evaluate the model’s performance without needing a separate validation dataset.\n\n---\n\n### **5. How can you measure the importance of features in a Random Forest model?**  \nFeature importance in Random Forest can be measured using:  \n1. **Mean Decrease in Impurity (Gini Importance):** Measures how much a feature reduces the impurity (e.g., Gini index) when used for splitting.  \n2. **Permutation Importance:** Randomly shuffles feature values and observes the decrease in model accuracy. If accuracy drops significantly, the feature is important.\n\n---\n\n### **6. Explain the working principle of a Bagging Classifier.**  \nA **Bagging Classifier** works by:  \n1. Generating multiple bootstrap samples from the training dataset.  \n2. Training a separate base model (e.g., Decision Tree) on each bootstrap sample.  \n3. Aggregating the predictions of all base models using **majority voting** (for classification) or **averaging** (for regression).  \nThis process reduces variance and prevents overfitting.\n\n---\n\n### **7. How do you evaluate a Bagging Classifier’s performance?**  \nA Bagging Classifier’s performance can be evaluated using:  \n- **Accuracy** (for classification problems)  \n- **Precision, Recall, and F1-score** (for imbalanced classification)  \n- **ROC-AUC Score** (for probabilistic classification)  \n- **Cross-validation** (to check stability across different data splits)\n\n---\n\n### **8. How does a Bagging Regressor work?**  \nA **Bagging Regressor**:  \n1. Creates multiple bootstrap samples from the original dataset.  \n2. Trains multiple regression models (e.g., Decision Trees) on these samples.  \n3. Averages the predictions from all models.  \nThis averaging process reduces variance, making the model more stable.\n\n---\n\n### **9. What is the main advantage of ensemble techniques?**  \nThe primary advantage is **improved model accuracy and generalization**. By combining multiple models, ensemble methods reduce **variance (Bagging), bias (Boosting), or both (Stacking)**, leading to better predictive performance.\n\n---\n\n### **10. What is the main challenge of ensemble methods?**  \nThe main challenges include:  \n- **Computational cost:** Training multiple models requires more resources.  \n- **Complexity:** Harder to interpret than a single model.  \n- **Risk of overfitting (in Boosting):** If not tuned properly, some ensembles (e.g., Boosting) can lead to overfitting.\n\n---\n\n### **11. Explain the key idea behind ensemble techniques.**  \nThe key idea is that **multiple weak models** (learners) can be combined to create a **stronger model**. This improves prediction accuracy, reduces variance, and prevents overfitting.\n\n---\n\n### **12. What is a Random Forest Classifier?**  \nA **Random Forest Classifier** is an ensemble model that builds multiple **Decision Trees** and combines their predictions using **majority voting**. It improves accuracy and reduces overfitting compared to a single Decision Tree.\n\n---\n\n### **13. What are the main types of ensemble techniques?**  \n1. **Bagging (Bootstrap Aggregating):** Reduces variance (e.g., Random Forest).  \n2. **Boosting:** Reduces bias by sequentially training models (e.g., AdaBoost, XGBoost).  \n3. **Stacking:** Uses different models and a meta-learner to improve predictions.\n\n---\n\n### **14. What is ensemble learning in machine learning?**  \nEnsemble learning is a technique that combines multiple models to achieve better performance than a single model.\n\n---\n\n### **15. When should we avoid using ensemble methods?**  \n- When the dataset is small, and a single model performs well.  \n- When interpretability is important.  \n- When computational resources are limited.\n\n---\n\n### **16. How does Bagging help in reducing overfitting?**  \nBagging reduces overfitting by **averaging multiple models trained on different bootstrap samples**, which lowers variance and improves generalization.\n\n---\n\n### **17. Why is Random Forest better than a single Decision Tree?**  \nA single Decision Tree is prone to **overfitting**, whereas Random Forest reduces overfitting by averaging multiple trees, making it more stable.\n\n---\n\n### **18. What is the role of bootstrap sampling in Bagging?**  \nBootstrap sampling randomly selects data points **with replacement** to create multiple training sets. This helps in training diverse models, reducing variance.\n\n---\n\n### **19. What are some real-world applications of ensemble techniques?**  \n- **Finance:** Credit risk assessment.  \n- **Healthcare:** Disease diagnosis.  \n- **Marketing:** Customer segmentation.  \n- **Cybersecurity:** Fraud detection.  \n\n---\n\n### **20. What is the difference between Bagging and Boosting?**  \n| Feature  | Bagging  | Boosting  |\n|----------|---------|---------|\n| Goal  | Reduce variance  | Reduce bias |\n| Training | Parallel (independent models) | Sequential (dependent models) |\n| Weighting | Equal for all models | More weight to misclassified samples |\n| Example | Random Forest | AdaBoost, XGBoost |\n",
      "metadata": {}
    },
    {
      "id": "8ad9b053-e516-4850-a818-37b3173a2878",
      "cell_type": "markdown",
      "source": "                                           PRACTICAL QUESTIONS ",
      "metadata": {}
    },
    {
      "id": "77c32e2f-6dc8-4ad9-ac1d-ebe83496512f",
      "cell_type": "markdown",
      "source": "\n# **1. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**\n# **Steps:**\n1. Import necessary libraries.\n2. Load a sample dataset (e.g., Iris dataset).\n3. Split the dataset into training and testing sets.\n4. Use `GridSearchCV` to find the best hyperparameters for the `RandomForestClassifier`.\n5. Train the model with optimal parameters and evaluate accuracy.\n\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define model\nrf = RandomForestClassifier()\n\n# Define hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\n# GridSearchCV\ngrid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Accuracy\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n### **2. Train a Bagging Regressor with different numbers of base estimators and compare performance**\n#### **Steps:**\n1. Load the Boston Housing dataset.\n2. Train multiple `BaggingRegressor` models with different numbers of estimators.\n3. Evaluate performance using **Mean Squared Error (MSE)**.\n\n#### **Code:**\n```python\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Different base estimators\nfor n in [10, 50, 100]:\n    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f\"Base estimators: {n}, MSE: {mean_squared_error(y_test, y_pred)}\")\n```\n\n---\n\n### **3. Train a Random Forest Classifier and analyze misclassified samples**\n#### **Steps:**\n1. Train a `RandomForestClassifier` on a dataset.\n2. Compare actual vs. predicted values and identify misclassified samples.\n\n#### **Code:**\n```python\nimport pandas as pd\n\n# Train Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\n# Find misclassified samples\nmisclassified = X_test[y_pred != y_test]\nprint(\"Misclassified Samples:\\n\", pd.DataFrame(misclassified))\n```\n\n---\n\n### **4. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**\n#### **Code:**\n```python\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Train a single Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\nprint(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\nbagging.fit(X_train, y_train)\nbag_pred = bagging.predict(X_test)\nprint(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))\n```\n\n---\n\n### **5. Train a Random Forest Classifier and visualize the confusion matrix**\n#### **Code:**\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Display confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n```\n\n---\n\n### **6. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**\n#### **Code:**\n```python\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models\nbase_learners = [\n    ('dt', DecisionTreeClassifier()),\n    ('svm', SVC(probability=True)),\n]\n\n# Define stacking model\nstacking = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\nstacking.fit(X_train, y_train)\nstack_pred = stacking.predict(X_test)\n\nprint(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack_pred))\n```\n\n---\n\n### **7. Train a Random Forest Classifier and print the top 5 most important features**\n#### **Code:**\n```python\nimport numpy as np\n\n# Train Random Forest\nrf.fit(X_train, y_train)\n\n# Get feature importance\nfeature_importance = np.argsort(rf.feature_importances_)[::-1]\ntop_features = feature_importance[:5]\n\nprint(\"Top 5 Important Features:\", data.feature_names[top_features])\n```\n\n---\n\n### **8. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**\n#### **Code:**\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Train Bagging Classifier\nbagging.fit(X_train, y_train)\nbag_pred = bagging.predict(X_test)\n\nprint(\"Precision:\", precision_score(y_test, bag_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, bag_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, bag_pred, average='weighted'))\n```\n\n---\n\n### **9. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**\n#### **Code:**\n```python\nfrom sklearn.metrics import roc_auc_score\n\n# Predict probabilities\ny_prob = rf.predict_proba(X_test)\n\n# Compute ROC-AUC\nprint(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob, multi_class='ovr'))\n```\n\n---\n\n### **10. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**\n#### **Code:**\n```python\nstacking2 = StackingClassifier(estimators=[('rf', RandomForestClassifier()), ('lr', LogisticRegression())],\n                               final_estimator=LogisticRegression())\nstacking2.fit(X_train, y_train)\nstack2_pred = stacking2.predict(X_test)\n\nprint(\"Stacking Classifier Accuracy (RF + LR):\", accuracy_score(y_test, stack2_pred))\n```\n",
      "metadata": {}
    },
    {
      "id": "834200c3-412e-430b-9d82-8b272875c3f5",
      "cell_type": "markdown",
      "source": "Here are detailed answers for **Practical Questions 31 to 45** from your assignment on **Ensemble Learning** with **Python implementation** for each:\n\n---\n\n### **31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**\n#### **Steps:**\n- Load a dataset (e.g., Iris dataset).\n- Perform **hyperparameter tuning** using `GridSearchCV`.\n- Train the best model and evaluate accuracy.\n\n#### **Code:**\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define model\nrf = RandomForestClassifier()\n\n# Hyperparameter tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\ngrid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\n# Accuracy\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **32. Train a Bagging Regressor with different numbers of base estimators and compare performance**\n#### **Code:**\n```python\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndata = fetch_california_housing()\nX, y = data.data, data.target\n\n# Train different Bagging Regressors\nfor n in [10, 50, 100]:\n    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    print(f\"Base estimators: {n}, MSE: {mean_squared_error(y, y_pred)}\")\n```\n\n---\n\n### **33. Train a Random Forest Classifier and analyze misclassified samples**\n#### **Code:**\n```python\nimport pandas as pd\n\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\n# Find misclassified samples\nmisclassified = X_test[y_pred != y_test]\nprint(\"Misclassified Samples:\\n\", pd.DataFrame(misclassified))\n```\n\n---\n\n### **34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**\n#### **Code:**\n```python\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Train Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ndt_pred = dt.predict(X_test)\nprint(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\nbagging.fit(X_train, y_train)\nbag_pred = bagging.predict(X_test)\nprint(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))\n```\n\n---\n\n### **35. Train a Random Forest Classifier and visualize the confusion matrix**\n#### **Code:**\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Display confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n```\n\n---\n\n### **36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**\n#### **Code:**\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models\nbase_learners = [\n    ('dt', DecisionTreeClassifier()),\n    ('svm', SVC(probability=True)),\n]\n\n# Stacking model\nstacking = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\nstacking.fit(X_train, y_train)\nstack_pred = stacking.predict(X_test)\n\nprint(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stack_pred))\n```\n\n---\n\n### **37. Train a Random Forest Classifier and print the top 5 most important features**\n#### **Code:**\n```python\nimport numpy as np\n\n# Train Random Forest\nrf.fit(X_train, y_train)\n\n# Get feature importance\nfeature_importance = np.argsort(rf.feature_importances_)[::-1]\ntop_features = feature_importance[:5]\n\nprint(\"Top 5 Important Features:\", data.feature_names[top_features])\n```\n\n---\n\n### **38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**\n#### **Code:**\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Train Bagging Classifier\nbagging.fit(X_train, y_train)\nbag_pred = bagging.predict(X_test)\n\nprint(\"Precision:\", precision_score(y_test, bag_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, bag_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, bag_pred, average='weighted'))\n```\n\n---\n\n### **39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**\n#### **Code:**\n```python\ndepths = [5, 10, 20, None]\n\nfor depth in depths:\n    rf = RandomForestClassifier(max_depth=depth, random_state=42)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    print(f\"Max Depth: {depth}, Accuracy: {accuracy_score(y_test, y_pred)}\")\n```\n\n---\n\n### **40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**\n#### **Code:**\n```python\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels = {'Decision Tree': DecisionTreeRegressor(), 'KNeighbors': KNeighborsRegressor()}\n\nfor name, estimator in models.items():\n    bagging = BaggingRegressor(base_estimator=estimator, n_estimators=50, random_state=42)\n    bagging.fit(X_train, y_train)\n    y_pred = bagging.predict(X_test)\n    print(f\"{name} - MSE: {mean_squared_error(y_test, y_pred)}\")\n```\n\n---\n\n### **41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**\n#### **Code:**\n```python\nfrom sklearn.metrics import roc_auc_score\n\n# Predict probabilities\ny_prob = rf.predict_proba(X_test)\n\n# Compute ROC-AUC\nprint(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob, multi_class='ovr'))\n```\n\n---\n\n### **42. Train a Bagging Classifier and evaluate its performance using cross-validation**\n#### **Code:**\n```python\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(bagging, X, y, cv=5)\nprint(\"Cross-validation scores:\", scores)\nprint(\"Mean Accuracy:\", scores.mean())\n```\n\n---\n\n### **43. Train a Random Forest Classifier and plot the Precision-Recall curve**\n#### **Code:**\n```python\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, _ = precision_recall_curve(y_test, y_prob[:, 1])\nplt.plot(recall, precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.show()\n```\n\n---\n\n### **44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**\n#### **(Same as Q36, but using RF instead of SVM)**\n\n---\n\n### **45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance**\n#### **Code:**\n```python\nfor bootstrap in [True, False]:\n    bagging = BaggingRegressor(n_estimators=50, bootstrap=bootstrap, random_state=42)\n    bagging.fit(X_train, y_train)\n    y_pred = bagging.predict(X_test)\n    print(f\"Bootstrap={bootstrap}, MSE: {mean_squared_error(y_test, y_pred)}\")\n```\n\n",
      "metadata": {}
    },
    {
      "id": "4f51adeb-e737-4a63-86e9-00ffef18ee3f",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "56c8d2f0-ef60-447d-9283-949a0fc54346",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cf2f3828-a148-4de7-b683-68d22a4d12f6",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}