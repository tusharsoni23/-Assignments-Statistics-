{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "de4754d1-9bae-4386-b09d-733a3c7d23fe",
      "cell_type": "markdown",
      "source": "\n\n### **1. What is Boosting in Machine Learning?**\n**Boosting** is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner. The key idea is to train models sequentially, where each new model focuses on correcting the errors of the previous ones. Popular boosting algorithms include:\n- **AdaBoost (Adaptive Boosting)**\n- **Gradient Boosting**\n- **XGBoost**\n- **CatBoost**\n- **LightGBM**\n\n---\n\n### **2. How does Boosting differ from Bagging?**\n| **Feature**  | **Boosting** | **Bagging** |\n|-------------|------------|------------|\n| Model Type  | Sequential | Parallel |\n| Goal | Reduce bias | Reduce variance |\n| Weak Learner | Focuses on misclassified samples | Trained independently |\n| Example Algorithms | AdaBoost, XGBoost | Random Forest |\n\nBoosting **reduces bias**, while bagging **reduces variance**.\n\n---\n\n### **3. What is the key idea behind AdaBoost?**\nThe key idea behind **AdaBoost** (Adaptive Boosting) is:\n1. Train a weak learner (e.g., decision stump).\n2. Assign higher weights to misclassified samples.\n3. Train the next weak learner on the updated dataset.\n4. Combine all weak learners using a weighted sum.\n\nThis process helps focus learning on **hard-to-classify samples**.\n\n---\n\n### **4. Explain the working of AdaBoost with an example**\n**Example:** Suppose we classify emails as spam or not spam.\n\n1. Train a decision stump on the dataset.\n2. If some emails are misclassified, increase their weights.\n3. Train a new weak learner focusing on these misclassified emails.\n4. Repeat the process for multiple iterations.\n5. Final prediction is made by combining all weak learners using weighted voting.\n\n---\n\n### **5. What is Gradient Boosting, and how is it different from AdaBoost?**\n**Gradient Boosting** builds models sequentially like AdaBoost but optimizes a **loss function** using **gradient descent** instead of adjusting sample weights.\n\n| **Feature** | **AdaBoost** | **Gradient Boosting** |\n|------------|-------------|------------------|\n| Weight Update | Assigns higher weights to misclassified samples | Minimizes loss using gradient descent |\n| Weak Learners | Decision Stumps (Shallow Trees) | Deeper Trees |\n| Error Handling | Weighted Majority Voting | Loss Function Optimization |\n\nGradient Boosting generally **performs better** than AdaBoost.\n\n---\n\n### **6. What is the loss function in Gradient Boosting?**\nThe loss function measures **how far predictions are from actual values**. Common loss functions:\n- **For Regression**: Mean Squared Error (MSE)\n- **For Classification**: Log Loss (Cross-Entropy)\n\nGradient Boosting minimizes this loss function using gradient descent.\n\n---\n\n### **7. How does XGBoost improve over traditional Gradient Boosting?**\nXGBoost (Extreme Gradient Boosting) improves upon Gradient Boosting by:\n- **Regularization**: L1 (Lasso) and L2 (Ridge) prevent overfitting.\n- **Tree Pruning**: Uses **depth-wise growth** instead of level-wise growth.\n- **Parallel Processing**: Faster training using multi-threading.\n- **Handling Missing Data**: Automatically finds optimal splits for missing values.\n\nXGBoost is **faster and more accurate** than traditional Gradient Boosting.\n\n---\n\n### **8. What is the difference between XGBoost and CatBoost?**\n| **Feature** | **XGBoost** | **CatBoost** |\n|------------|------------|-------------|\n| Data Type | Works best with numerical data | Works well with categorical data |\n| Speed | Faster than traditional Gradient Boosting | Faster on categorical data |\n| Handling Categorical Features | Requires encoding (One-Hot, Label Encoding) | Handles categorical data natively |\n| Industry Use | General-purpose ML tasks | NLP, e-commerce, finance |\n\nCatBoost is **better for categorical data**, while XGBoost is a **general-purpose** boosting algorithm.\n\n---\n\n### **9. What are some real-world applications of Boosting techniques?**\nBoosting is used in:\nâœ… **Fraud Detection** (Banking & Finance)  \nâœ… **Disease Prediction** (Healthcare)  \nâœ… **Recommendation Systems** (E-commerce)  \nâœ… **Spam Filtering** (Email Services)  \nâœ… **Customer Churn Prediction** (Telecom)  \nâœ… **Stock Price Prediction** (Finance)  \n\n---\n\n### **10. How does regularization help in XGBoost?**\nRegularization helps prevent **overfitting** in XGBoost:\n- **L1 Regularization (Lasso)**: Shrinks less important features to zero.\n- **L2 Regularization (Ridge)**: Reduces model complexity by penalizing large weights.\n- **Gamma Parameter**: Prunes trees by setting a minimum loss reduction required for a split.\n\nThis ensures **better generalization**.\n\n---\n\n### **11. What are some hyperparameters to tune in Gradient Boosting models?**\nImportant hyperparameters:\n- **n_estimators**: Number of trees (More trees = better fit but slower training).\n- **learning_rate**: Step size in gradient descent (Lower values prevent overfitting).\n- **max_depth**: Tree depth (Controls complexity).\n- **min_samples_split**: Minimum samples required to split a node.\n- **subsample**: Fraction of data used per tree (Prevents overfitting).\n\nTuning these using **GridSearchCV** improves model performance.\n\n---\n\n### **12. What is the concept of Feature Importance in Boosting?**\n**Feature Importance** shows how much each feature contributes to predictions. It is computed based on:\n- **Gini Importance (Split-based importance)**: Measures how often a feature is used for splitting.\n- **Permutation Importance**: Measures accuracy drop when a feature is shuffled.\n\nFeature Importance helps with:\n- **Feature Selection**\n- **Reducing Overfitting**\n- **Improving Interpretability**\n\n---\n\n### **13. Why is CatBoost efficient for categorical data?**\nCatBoost is efficient because:\n1. **Handles categorical variables natively** (No need for One-Hot Encoding).\n2. **Uses Ordered Boosting** to reduce target leakage.\n3. **Efficient GPU training**, making it faster than XGBoost for certain tasks.\n",
      "metadata": {}
    },
    {
      "id": "40d867f1-5927-4067-9b9c-de648437b740",
      "cell_type": "code",
      "source": "                                   PRACTICAL QUESTIONS ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "452c651e-c2b0-4948-97dc-953193697761",
      "cell_type": "code",
      "source": "Here are Python programs for all the **Boosting Practical Questions (14 to 30)** from your assignment:\n\n---\n\n### **14. Train an AdaBoost Classifier on a sample dataset and print model accuracy**\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Generate dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train AdaBoost Classifier\nclf = AdaBoostClassifier(n_estimators=50, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict and print accuracy\ny_pred = clf.predict(X_test)\nprint(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)**\n```python\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_absolute_error\n\n# Generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train AdaBoost Regressor\nregressor = AdaBoostRegressor(n_estimators=50, random_state=42)\nregressor.fit(X_train, y_train)\n\n# Predict and evaluate MAE\ny_pred = regressor.predict(X_test)\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n```\n\n---\n\n### **16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance**\n```python\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n# Load dataset\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\n\n# Train Gradient Boosting Classifier\nclf = GradientBoostingClassifier(n_estimators=100, random_state=42)\nclf.fit(X, y)\n\n# Print feature importances\nprint(\"Feature Importances:\", clf.feature_importances_)\n```\n\n---\n\n### **17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score**\n```python\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\n\n# Train Gradient Boosting Regressor\nregressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\nregressor.fit(X_train, y_train)\n\n# Predict and evaluate RÂ² score\ny_pred = regressor.predict(X_test)\nprint(\"R-Squared Score:\", r2_score(y_test, y_pred))\n```\n\n---\n\n### **18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting**\n```python\nfrom xgboost import XGBClassifier\n\n# Train XGBoost Classifier\nxgb_clf = XGBClassifier(n_estimators=100, random_state=42)\nxgb_clf.fit(X_train, y_train)\n\ny_pred_xgb = xgb_clf.predict(X_test)\n\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n```\n\n---\n\n### **19. Train a CatBoost Classifier and evaluate using F1-Score**\n```python\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import f1_score\n\n# Train CatBoost Classifier\ncat_clf = CatBoostClassifier(iterations=100, silent=True)\ncat_clf.fit(X_train, y_train)\n\ny_pred_cat = cat_clf.predict(X_test)\nprint(\"F1 Score:\", f1_score(y_test, y_pred_cat))\n```\n\n---\n\n### **20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)**\n```python\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Train XGBoost Regressor\nxgb_reg = XGBRegressor(n_estimators=100, random_state=42)\nxgb_reg.fit(X_train, y_train)\n\ny_pred_xgb = xgb_reg.predict(X_test)\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_xgb))\n```\n\n---\n\n### **21. Train an AdaBoost Classifier and visualize feature importance**\n```python\nimport matplotlib.pyplot as plt\n\nplt.bar(range(X_train.shape[1]), clf.feature_importances_)\nplt.xlabel(\"Feature Index\")\nplt.ylabel(\"Importance\")\nplt.title(\"Feature Importance in AdaBoost\")\nplt.show()\n```\n\n---\n\n### **22. Train a Gradient Boosting Regressor and plot learning curves**\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntrain_errors, test_errors = [], []\nfor m in range(1, len(X_train)):\n    regressor.fit(X_train[:m], y_train[:m])\n    y_train_predict = regressor.predict(X_train[:m])\n    y_test_predict = regressor.predict(X_test)\n    train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n    test_errors.append(mean_squared_error(y_test, y_test_predict))\n\nplt.plot(np.sqrt(train_errors), \"r-+\", label=\"Training error\")\nplt.plot(np.sqrt(test_errors), \"b-\", label=\"Test error\")\nplt.legend()\nplt.show()\n```\n\n---\n\n### **23. Train an XGBoost Classifier and visualize feature importance**\n```python\nfrom xgboost import plot_importance\n\nplot_importance(xgb_clf)\nplt.show()\n```\n\n---\n\n### **24. Train a CatBoost Classifier and plot the confusion matrix**\n```python\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_test, y_pred_cat)\nsns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix for CatBoost\")\nplt.show()\n```\n\n---\n\n### **25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy**\n```python\nfor n in [10, 50, 100, 200]:\n    clf = AdaBoostClassifier(n_estimators=n, random_state=42)\n    clf.fit(X_train, y_train)\n    print(f\"Accuracy with {n} estimators:\", accuracy_score(y_test, clf.predict(X_test)))\n```\n\n---\n\n### **26. Train a Gradient Boosting Classifier and visualize the ROC curve**\n```python\nfrom sklearn.metrics import roc_curve, auc\n\ny_scores = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, _ = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n```\n\n---\n\n### **27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV**\n```python\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\"learning_rate\": [0.01, 0.1, 0.2, 0.3]}\ngrid_search = GridSearchCV(XGBRegressor(), param_grid, scoring=\"neg_mean_squared_error\", cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best learning rate:\", grid_search.best_params_)\n```\n\n---\n\n### **28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting**\n```python\ncat_clf = CatBoostClassifier(class_weights=[0.7, 0.3], iterations=100, silent=True)\ncat_clf.fit(X_train, y_train)\nprint(\"Accuracy with class weighting:\", accuracy_score(y_test, cat_clf.predict(X_test)))\n```\n\n---\n\n### **29. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss**\n```python\nfrom sklearn.metrics import log_loss\n\ny_pred_probs = xgb_clf.predict_proba(X_test)\nprint(\"Log Loss:\", log_loss(y_test, y_pred_probs))\n## **30 Train an AdaBoost Classifier and analyze the effect of different learning rate\nHereâ€™s a Python program to **train an AdaBoost Classifier** and analyze the effect of different **learning rates** on model accuracy:\n\n---\n\n### **Program: Train AdaBoost with Different Learning Rates**\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Generate a classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Different learning rates to test\nlearning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n\naccuracies = []\n\n# Train AdaBoost with different learning rates and measure accuracy\nfor lr in learning_rates:\n    clf = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    accuracies.append(acc)\n    print(f\"Learning Rate: {lr}, Accuracy: {acc:.4f}\")\n\n# Plot learning rate vs accuracy\nplt.plot(learning_rates, accuracies, marker='o', linestyle='-', color='b')\nplt.xscale('log')  # Log scale for better visualization\nplt.xlabel(\"Learning Rate\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Effect of Learning Rate on AdaBoost Accuracy\")\nplt.grid(True)\nplt.show()\n```\n\n---\n\n### **Analysis: Effect of Learning Rate**\n- **Very Small Learning Rate (0.001, 0.01)**: The model learns **too slowly**, leading to **underfitting**.\n- **Optimal Learning Rate (0.1 - 1.0)**: These values typically **balance bias and variance**, leading to **good accuracy**.\n- **Very High Learning Rate (2.0)**: The model learns **too aggressively**, causing **overfitting or divergence**.\n\nThis analysis helps in **choosing the best learning rate** for AdaBoost models.\n\nLet me know if you need any modifications! ðŸš€ðŸ˜Š\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}