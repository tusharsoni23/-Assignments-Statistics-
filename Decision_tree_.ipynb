{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "64634ecb-ef6f-4fc8-82a6-0d4f3635dbed",
      "cell_type": "markdown",
      "source": "Here are the detailed answers to all the theory questions from your assignment:\n\n---\n\n### **1. What is a Decision Tree, and how does it work?**\nA **Decision Tree** is a supervised learning algorithm used for classification and regression tasks. It is a tree-like model where:\n- Each internal node represents a decision based on a feature.\n- Each branch represents an outcome of the decision.\n- Each leaf node represents a class label (classification) or a continuous value (regression).\n\n**How it works:**\n1. The dataset is split into subsets based on feature values.\n2. Splitting continues recursively until a stopping condition is met (e.g., minimum samples per node, tree depth, etc.).\n3. The best split is determined using impurity measures such as **Gini Impurity** or **Entropy**.\n\n---\n\n### **2. What are impurity measures in Decision Trees?**\nImpurity measures determine how well a dataset is split into classes. The two main impurity measures are:\n- **Gini Impurity**: Measures the probability of misclassification.\n- **Entropy**: Measures the randomness in the dataset.\n\nA pure node has an impurity measure of zero, meaning all samples belong to one class.\n\n---\n\n### **3. What is the mathematical formula for Gini Impurity?**\nThe **Gini Impurity** for a node is given by:\n\n\\[\nGini = 1 - \\sum_{i=1}^{c} p_i^2\n\\]\n\nWhere:\n- \\( p_i \\) is the probability of a sample belonging to class \\( i \\).\n- \\( c \\) is the number of classes.\n\nA lower Gini value indicates a purer node.\n\n---\n\n### **4. What is the mathematical formula for Entropy?**\nThe **Entropy** for a node is calculated as:\n\n\\[\nEntropy = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n\\]\n\nWhere:\n- \\( p_i \\) is the probability of a sample belonging to class \\( i \\).\n- \\( c \\) is the number of classes.\n\nEntropy is highest when classes are evenly distributed and lowest when nodes are pure.\n\n---\n\n### **5. What is Information Gain, and how is it used in Decision Trees?**\n**Information Gain (IG)** measures the reduction in impurity when a dataset is split. It is calculated as:\n\n\\[\nIG = Entropy(parent) - \\sum \\left(\\frac{|child|}{|parent|} \\times Entropy(child)\\right)\n\\]\n\nA split with higher **Information Gain** is preferred, as it reduces uncertainty in the dataset.\n\n---\n\n### **6. What is the difference between Gini Impurity and Entropy?**\n| **Gini Impurity** | **Entropy** |\n|------------------|-----------|\n| Measures misclassification probability | Measures randomness (uncertainty) |\n| Computationally faster | More complex due to logarithms |\n| Lower values indicate purer splits | Higher values indicate greater disorder |\n\nBoth lead to similar decision trees, but **Gini is computationally more efficient**.\n\n---\n\n### **7. What is the mathematical explanation behind Decision Trees?**\nA Decision Tree recursively splits data using:\n1. **Impurity Measures** (Gini or Entropy).\n2. **Best Split Selection**:\n   - The feature and threshold that maximize **Information Gain**.\n3. **Stopping Conditions**:\n   - Minimum samples per node, maximum depth, etc.\n4. **Pruning Techniques**:\n   - Pre-Pruning and Post-Pruning to prevent overfitting.\n\nMathematically, decision trees optimize:\n\n\\[\nSplit_{best} = \\arg \\max_{split} \\text{Information Gain}\n\\]\n\n---\n\n### **8. What is Pre-Pruning in Decision Trees?**\n**Pre-Pruning** (Early Stopping) limits tree growth **before** fully developing it by:\n- Setting a **maximum depth**.\n- Restricting the **minimum samples per split**.\n- Limiting the **minimum impurity decrease**.\n\nThis prevents **overfitting** but may lead to **underfitting**.\n\n---\n\n### **9. What is Post-Pruning in Decision Trees?**\n**Post-Pruning** removes unnecessary branches **after** the tree is fully grown. It works by:\n1. Growing the tree completely.\n2. Removing branches with little contribution using **Cost Complexity Pruning (CCP)**.\n\nPost-pruning improves **generalization** but requires validation.\n\n---\n\n### **10. What is the difference between Pre-Pruning and Post-Pruning?**\n| **Pre-Pruning** | **Post-Pruning** |\n|---------------|---------------|\n| Stops tree growth early | Prunes after full growth |\n| May cause underfitting | Reduces overfitting |\n| Computationally efficient | Requires additional validation |\n\nPost-pruning is generally more effective but computationally expensive.\n\n---\n\n### **11. What is a Decision Tree Regressor?**\nA **Decision Tree Regressor** predicts **continuous values** instead of class labels. It works similarly to classification trees but minimizes **Mean Squared Error (MSE)** instead of impurity measures.\n\n---\n\n### **12. What are the advantages and disadvantages of Decision Trees?**\n✅ **Advantages:**\n- Simple and easy to interpret.\n- No need for feature scaling.\n- Handles both categorical and numerical data.\n- Can model non-linear relationships.\n\n❌ **Disadvantages:**\n- Prone to **overfitting** (especially deep trees).\n- Sensitive to **noisy data**.\n- High variance (small data changes may alter tree structure).\n\n---\n\n### **13. How does a Decision Tree handle missing values?**\nDecision Trees handle missing values by:\n- Ignoring missing values during splits.\n- Using surrogate splits (alternative splits for missing data).\n- Imputing missing values with the most frequent class (classification) or mean (regression).\n\n---\n\n### **14. How does a Decision Tree handle categorical features?**\nCategorical features are handled by:\n- **One-Hot Encoding** (if using algorithms like CART).\n- **Label Encoding** (when dealing with ordinal data).\n- **Binary splits** (splitting categories into groups).\n\n---\n\n### **15. What are some real-world applications of Decision Trees?**\n✅ **Real-world applications:**\n- **Healthcare**: Disease diagnosis.\n- **Finance**: Loan approval predictions.\n- **Marketing**: Customer segmentation.\n- **Fraud Detection**: Identifying fraudulent transactions.\n- **Manufacturing**: Predicting machine failures.\n\n",
      "metadata": {}
    },
    {
      "id": "9455a9d1-4655-4df0-b64f-513e9eef4483",
      "cell_type": "code",
      "source": "                                          PRACTICAL QUESTIONS ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "23234820-88cc-443e-83f4-f6f56d4fba28",
      "cell_type": "markdown",
      "source": "\n\n### **16. Train a Decision Tree Classifier on the Iris dataset and print the model accuracy**\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\n\n# Predict and print accuracy\ny_pred = clf.predict(X_test)\nprint(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **17. Train a Decision Tree Classifier using Gini Impurity and print feature importances**\n```python\nclf = DecisionTreeClassifier(criterion=\"gini\")\nclf.fit(X_train, y_train)\n\n# Print feature importances\nprint(\"Feature Importances:\", clf.feature_importances_)\n```\n\n---\n\n### **18. Train a Decision Tree Classifier using Entropy and print the model accuracy**\n```python\nclf = DecisionTreeClassifier(criterion=\"entropy\")\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **19. Train a Decision Tree Regressor on a housing dataset and evaluate using MSE**\n```python\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Decision Tree Regressor\nregressor = DecisionTreeRegressor()\nregressor.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = regressor.predict(X_test)\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n```\n\n---\n\n### **20. Train a Decision Tree Classifier and visualize it using Graphviz**\n```python\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names,\n                           filled=True, rounded=True, special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"decision_tree\")  # Saves tree as a file\ngraph  # Display the tree\n```\n\n---\n\n### **21. Train a Decision Tree Classifier with max depth = 3 and compare accuracy with a fully grown tree**\n```python\nclf_limited = DecisionTreeClassifier(max_depth=3)\nclf_limited.fit(X_train, y_train)\n\ny_pred_limited = clf_limited.predict(X_test)\nprint(\"Accuracy with max depth = 3:\", accuracy_score(y_test, y_pred_limited))\nprint(\"Accuracy with fully grown tree:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **22. Train a Decision Tree Classifier using min_samples_split=5 and compare with default tree**\n```python\nclf_min_samples = DecisionTreeClassifier(min_samples_split=5)\nclf_min_samples.fit(X_train, y_train)\n\ny_pred_min_samples = clf_min_samples.predict(X_test)\nprint(\"Accuracy with min_samples_split=5:\", accuracy_score(y_test, y_pred_min_samples))\nprint(\"Accuracy with default tree:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **23. Apply feature scaling before training a Decision Tree Classifier and compare accuracy**\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nclf_scaled = DecisionTreeClassifier()\nclf_scaled.fit(X_train_scaled, y_train)\n\ny_pred_scaled = clf_scaled.predict(X_test_scaled)\nprint(\"Accuracy with scaling:\", accuracy_score(y_test, y_pred_scaled))\nprint(\"Accuracy without scaling:\", accuracy_score(y_test, y_pred))\n```\n\n---\n\n### **24. Train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification**\n```python\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr_clf = OneVsRestClassifier(DecisionTreeClassifier())\novr_clf.fit(X_train, y_train)\n\ny_pred_ovr = ovr_clf.predict(X_test)\nprint(\"Accuracy with OvR:\", accuracy_score(y_test, y_pred_ovr))\n```\n\n---\n\n### **25. Train a Decision Tree Classifier and display the feature importance scores**\n```python\nprint(\"Feature Importances:\", clf.feature_importances_)\n```\n\n---\n\n### **26. Train a Decision Tree Regressor with max_depth=5 and compare with an unrestricted tree**\n```python\nregressor_limited = DecisionTreeRegressor(max_depth=5)\nregressor_limited.fit(X_train, y_train)\n\ny_pred_limited = regressor_limited.predict(X_test)\nprint(\"MSE with max_depth=5:\", mean_squared_error(y_test, y_pred_limited))\nprint(\"MSE with unrestricted tree:\", mean_squared_error(y_test, y_pred))\n```\n\n---\n\n### **27. Train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy**\n```python\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas\n\nfor alpha in ccp_alphas:\n    pruned_clf = DecisionTreeClassifier(ccp_alpha=alpha)\n    pruned_clf.fit(X_train, y_train)\n    y_pred_pruned = pruned_clf.predict(X_test)\n    print(f\"Alpha: {alpha}, Accuracy: {accuracy_score(y_test, y_pred_pruned)}\")\n```\n\n---\n\n### **28. Train a Decision Tree Classifier and evaluate performance using Precision, Recall, and F1-Score**\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprint(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n```\n\n---\n\n### **29. Train a Decision Tree Classifier and visualize the confusion matrix using seaborn**\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=iris.target_names, yticklabels=iris.target_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n```\n\n---\n\n### **30. Train a Decision Tree Classifier and use GridSearchCV to find optimal values for max_depth and min_samples_split**\n```python\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 5, 10, None],\n    'min_samples_split': [2, 5, 10]\n}\n\ngrid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Accuracy:\", grid_search.best_score_)\n```\n\n",
      "metadata": {}
    },
    {
      "id": "08a8ba33-71e9-4abf-a179-f6693dfd0584",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d7652c9-0e94-4b2c-8825-d9e9d593e369",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4c654ecd-2722-48e8-a2aa-141b0057e024",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5b64e231-dd93-475f-a107-a96765c7b8d1",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}